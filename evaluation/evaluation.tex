\chapter{Evaluation}

\section{Introduction}
This chapter will present the results of this research. First, the evaluation metrics that were used to evaluate the classifiers will be introduced. The results of the evaluation of the thirteen classifiers with the seven different feature representations will be outlined. The methods used to evaluate the recommender system will be defined. Finally, the results of this evaluation will be presented. 

\section{Evaluation Metrics for Classification}

\subsection{Confusion Matrix}
A confusion matrix is a technique commonly used for evaluating the performance of a classification model. It presents a summary of the prediction results of the classifier. A confusion matrix consists of four different combinations of the actual values versus the predicted values (Table ~\ref{Table:confusionmatrix}). 

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Confusion Matrix.}
\label{Table:confusionmatrix}
\begin{tabular}{c|c|c|}
\cline{2-3}
 & \multicolumn{1}{l|}{Actual Positive} & \multicolumn{1}{l|}{Actual Negative} \\ \hline
\multicolumn{1}{|l|}{Predicted Positive} & TP & FP \\ \hline
\multicolumn{1}{|l|}{Predicted Negative} & FN & TN \\ \hline
\end{tabular}
\end{table}

The values in a confusion matrix are:
\begin{itemize}
    \item \textbf{True Positive (TP)}\newline
    You have a True Positive value if your algorithm classified an item as positive and this was correct.
    \item \textbf{False Positive (FP)}\newline
    You have a False Positive value if your algorithm classified an item as positive but the value was actually negative.
    \item \textbf{False Negative (FN)}\newline
    You have a False Negative value if your algorithm classified an item as negative but the value was actually positive.
    \item \textbf{True Negative (TN)}\newline
    You have a True Negative value if your algorithm classified an item as negative and this was correct.
\end{itemize}

The confusion matrix is useful for calculating the precision, recall, accuracy and f1-score of a classification algorithm. These four metrics were used to evaluate the performance of the classifiers.

\subsection{Precision}
Precision is the percentage of positive identifications that were classified correctly.
\begin{equation}
    Precision\ =\ \cfrac{True\ Positives}{True\ Positives\ +\ False\ Positives}
\end{equation}
The higher the precision score the better. A low precision score can indicate that there is a large number of false positives. A high precision score indicates that the majority of what was classified was classified correctly. You could, however, have a very high precision score in the case where only a small portion of samples were actually classified but of these the majority were classified correctly. In this case, the precision score is high but the recall would be very low.

\subsection{Recall}
Recall is the percentage of all actual positives that were classified correctly.
\begin{equation}
    Recall\ =\ \cfrac{True\ Positives}{True\ Positives\ +\ False\ Negatives}
\end{equation}
The higher the recall score the better. A low recall value can indicate that there are a large number of false negatives. A high recall value indicates that the majority of what should have been classified was classified correctly. You could, however, have very high recall and low precision if everything was classified. All the samples that should have been classified are classified correctly, giving the high recall. But a lot of extra samples are classified that shouldn't have been classified, giving a low precision.

\subsection{Accuracy}
Accuracy is the total number of predictions the classifier got right. It is the percentage of the tweets that were classified correctly.
\begin{equation}
    Accuracy\ =\ \cfrac{Number\ of\ Correct\ Predictions}{Total\ Number\ of\ Predictions}
\end{equation}
The higher the accuracy score the better, but a high accuracy does not always give an accurate representation of the situation if there is a major class imbalance.

\subsection{F1-Score}
F1-Score is the harmonic mean of precision and recall. It is also called the F-Score or F-Measure. 
\begin{equation}
    F1-Score\ =\ 2 \times\ \cfrac{Precision\ \times\ Recall}{Precision\ +\ Recall}
\end{equation}
The higher the F1-Score the better. A high F1-Score indicates a good balance between precision and recall.

\subsection{Multi-Class Classification}
Multi-Class Classification is any classification task that involves two or more classes. This research tackles a multi-class classification problem. We aim to classify tweets as one of the following three classes: 'Review', 'Some Content', or 'Irrelevant'. For a binary classification problem, the precision, recall, f1-score and accuracy scores can be calculated simply with the above formulas. For multi-class classification, there needs to be a way of getting an average metric over the three classes, rather than an individual class score.
There are three ways that precision, recall, f1-score and accuracy can be calculated for this kind of multi-class classification problem:
\begin{enumerate}
    \item \textbf{Micro Average:}\newline
    The micro average calculates the metrics globally. It aggregates the contributions of all classes, counting the total true positives, false negatives and false positives, to calculate the overall average scores.  
    \item \textbf{Macro Average:}\newline
    The macro average calculates the metrics independently for each class. It then calculates the unweighted mean of the scores of the three classes.
    \item \textbf{Weighted Average:}\newline
    The weighted average calculates the metrics independently for each class like the macro average does. It then calculates the weighted average of the scores of the three classes. This differs from the macro average in that it takes class imbalance into account. Class imbalance means that there is a significant amount more or less samples in one class. In our case, there are significantly more tweets labelled as irrelevant than there are labelled as reviews (Table ~\ref{Table:tweetlabels}).
\end{enumerate}

The metrics listed in the tables below (Tables ~\ref{Table:precision},~\ref{Table:recall},~\ref{Table:accuracy},~\ref{Table:f1score}) were calculated with the weighted average.

\section{Classification Results and Analysis}

In this section, the results and evaluation of the classifiers will be presented. The classifiers evaluated include:
\begin{enumerate}
    \item Random Forest (RF) Classifier.
    \item Decision Tree (DT) Classifier.
    \item Multi Layer Perceptron (MLP) Classifier.
    \item Logistic Regression (LR) Classifier.    
    \item Support Vector Machine (SVM) Classifier.
    \item K Nearest Neighbours (KNN) Classifier.
    \item Gaussian Process (GP) Classifier.
    \item AdaBoost (AB) Classifier.
    \item Gaussian Naïve Bayes (GNB) Classifier.
    \item Multinomial Naïve Bayes (MNB) Classifier.
    \item Bernouilli Naïve Bayes (BNB) Classifier.
    \item Quadratic Discriminant Analysis (QDA) Classifier.
    \item Linear Discriminant Analysis (LDA) Classifier.
\end{enumerate}

These classifiers were evaluated with seven different feature representations defined below:
\begin{enumerate}
    \item Unigram BOW.
    \item Unigram TF-IDF.
    \item Bigram TF-IDF.
    \item Trigram TF-IDF.
    \item Unigram TF-IDF with stop words removed.
    \item Word2Vec.
    \item Doc2Vec.
\end{enumerate}

Taking the thirteen classifiers and seven feature representations there are 91 different combinations. Each was evaluated in terms of precision, recall, accuracy and f1-score.

The set of 3115 tweets was divided into a training set and a testing set with an 80:20 split. The label distribution of the data is shown in the table below (Table ~\ref{Table:tweetlabels}).

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Distribution of Tweet Labels.}
\label{Table:tweetlabels}
\begin{tabular}{|c|c|}
\hline
\textbf{Label} & \textbf{Tweet Count} \\ \hline
Review         & 566                  \\ \hline
Some Content   & 910                  \\ \hline
Irrelevant     & 1639                 \\ \hline
\end{tabular}
\end{table}

\subsection{Precision}

In terms of precision, the QDA Classifier with unigram TF-IDF and no stop words feature representation was the best performing classifier achieving a precision score of 0.79 (Table ~\ref{Table:precision}). This indicates that the QDA Classifier had a high percentage of positive identifications that were classified correctly. The QDA Classifier with unigram TF-IDF feature representation, this time with stop words included achieved the next highest precision score of 0.76. This further confirmed that unigram feature representation seems to work best with the QDA Classifier. The RF Classifier and the SVM Classifier, both with unigram TF-IDF feature representations were the two next best performing classifiers both achieving precision values of 0.74.

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Precision of Classifiers for different Feature Representations.}
\label{Table:precision}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.72 & 0.74 & 0.7 & 0.69 & 0.73 & 0.71 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.57 & 0.58 & 0.58 & 0.59 & 0.63 & 0.54 & 0.56  \\ \hline
\textbf{MLP} & 0.71 & 0.71 & 0.7 & 0.7 & 0.7 & 0.71 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.56 & 0.74 & 0.73 & 0.71 & 0.71 & 0.71 & 0.62  \\ \hline
\textbf{LR} & 0.72 & 0.72 & 0.7 & 0.68 & 0.7 & 0.71 & 0.61  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.53 & 0.62 & 0.62 & 0.62 & 0.61 & 0.67 & 0.54  \\ \hline
\textbf{GP} & 0.72 & 0.72 & 0.71 & 0.69 & 0.7 & 0.72 & 0.61  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.63 & 0.62 & 0.63 & 0.62 & 0.64 & 0.63 & 0.57  \\ \hline
\textbf{GNB} & 0.6 & 0.6 & 0.63 & 0.63 & 0.59 & 0.64 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.71} & 0.67 & 0.68 & 0.66 & 0.67 & 0.32 & 0.35    \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.73 & 0.73 & 0.71 & 0.71 & 0.72 & 0.65 & 0.58  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.72 & 0.76 & 0.73 & 0.67 & \textcolor{red}{0.79} & 0.59 & 0.57  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.63 & 0.64 & 0.65 & 0.63 & 0.62 & 0.69 & 0.66  \\ \hline
\end{tabular}}
\end{table}

It is interesting to compare our precision scores to those in the study by A.Rane and A.Kumar \cite{Rane2018}, in which they investigated the performance of various classifiers in classifying the sentiment of Twitter reviews about airlines. They found that the RF Classifier achieved the highest precision score, followed first by the AdaBoost Classifier and then the SVM Classifier. They did not include the QDA Classifier in their evaluation. Our results do not fully confirm or contradict the results of A.Rane and A.Kumar \cite{Rane2018}. Two out of three of our top performing classifiers agree, the RF classifier and the SVM classifier. Their precision scores are higher than ours for all the classifiers that they implemented, at approximately 0.8 compared to ours at approximately 0.7. A likely reason for this is that they trained their classifiers on a much larger dataset, a total of 14640 tweets, while our classifiers were trained on a smaller set of 3116 tweets. Supervised machine learning methods tend to perform the best on large datasets. Our results could possibly be improved by collecting and annotating a larger dataset.

\subsection{Recall}

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Recall of Classifiers for different Feature Representations.}
\label{Table:recall}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.7 & 0.71 & 0.7 & 0.69 & 0.72 & 0.69 & 0.64  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.61 & 0.6 & 0.61 & 0.62 & 0.65 & 0.56 & 0.59  \\ \hline
\textbf{MLP} & 0.72 & 0.72 & 0.71 & 0.71 & 0.71 & 0.72 & 0.62  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.59 & \textcolor{red}{0.74} & \textcolor{red}{0.74} & 0.72 & 0.72 & 0.72 & 0.64  \\ \hline
\textbf{LR} & 0.72 & 0.72 & 0.71 & 0.7 & 0.71 & 0.72 & 0.64  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.61 & 0.66 & 0.66 & 0.65 & 0.65 & 0.68 & 0.61  \\ \hline
\textbf{GP} & 0.73 & 0.73 & 0.72 & 0.71 & 0.71 & 0.73 & 0.64  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.65 & 0.64 & 0.65 & 0.65 & 0.66 & 0.64 & 0.61  \\ \hline
\textbf{GNB} & 0.48 & 0.48 & 0.47 & 0.45 & 0.48 & 0.56 & 0.49  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.66} & 0.68 & 0.69 & 0.67 & 0.68 & 0.57 & 0.59  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.71 & 0.71 & 0.66 & 0.62 & 0.71 & 0.56 & 0.54  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.26 & 0.23 & 0.24 & 0.56 & 0.26 & 0.61 & 0.68  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.61 & 0.61 & 0.61 & 0.59 & 0.59 & 0.69 & 0.67  \\ \hline
\end{tabular}}
\end{table}

The SVM Classifier was the best performing classifier with regard to recall, achieving a recall score of 0.74 (Table ~\ref{Table:recall}). The unigram TF-IDF and bigram TF-IDF feature representations performed equally well. This again partly agrees with A.Rane and A.Kumar \cite{Rane2018}, who found that the SVM Classifier was their second best performing classifier in terms of recall, with the RF classifier again coming out on top. Our scores are again slightly lower than those of A.Rane and A.Kumar \cite{Rane2018}.

Notice that the QDA Classifier which performed the best in precision performs pretty poorly in terms of recall. This is a common issue and is why we have also evaluated the classifiers with the f1-score. The f1-score conveys the balance between precision and recall giving us a better idea of the actual performance of the classifier.

The GP Classifier was the next highest performing classifier, achieving a recall score of 0.73, with unigram TF-IDF feature representation. This is interesting as I have not come across any studies where a GP Classifier performs well in classifying tweets. The GP Classifier also performed well in precision achieving a precision score of 0.72.  

\subsection{F1-Score}

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{F1-Score of Classifiers for different Feature Representations.}
\label{Table:f1score}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.65 & 0.65 & 0.64 & 0.64 & 0.69 & 0.63 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.57 & 0.59 & 0.58 & 0.6 & 0.63 & 0.54 & 0.56  \\ \hline
\textbf{MLP} & 0.71 & 0.7 & 0.69 & 0.69 & 0.69 & 0.71 & 0.57  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.45 & \textcolor{red}{0.73} & \textcolor{red}{0.73} & 0.71 & 0.71 & 0.71 & 0.63  \\ \hline
\textbf{LR} & 0.71 & 0.7 & 0.69 & 0.68 & 0.69 & 0.71 & 0.6  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.49 & 0.62 & 0.62 & 0.62 & 0.61 & 0.67 & 0.55  \\ \hline
\textbf{GP} & 0.71 & 0.71 & 0.72 & 0.7 & 0.7 & 0.72 & 0.61  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.63 & 0.62 & 0.63 & 0.62 & 0.63 & 0.63 & 0.59  \\ \hline
\textbf{GNB} & 0.5 & 0.51 & 0.49 & 0.47 & 0.5 & 0.58 & 0.51  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.67} & 0.63 & 0.66 & 0.65 & 0.64 & 0.41 & 0.44 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.72 & 0.72 & 0.67 & 0.64 & 0.71 & 0.58 & 0.55  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.23 & 0.19 & 0.19 & 0.48 & 0.23 & 0.55 & 0.61  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.62 & 0.62 & 0.62 & 0.6 & 0.6 & 0.69 & 0.66  \\ \hline
\end{tabular}}
\end{table}

Again, the SVM Classifier achieved the highest f1-score of 0.73 (Table ~\ref{Table:f1score}), with the unigram and bigram TF-IDF feature representations again performing equally well. This high f1-score indicates that the SVM Classifier has a good balance of precision and recall. This confirms the results of both Rathi et al. \cite{Raithi2018}, and A.Rane and A.Kumar \cite{Rane2018} who both found in their studies that the SVM Classifier performed well in classifying tweets. However, it contradicts the results of Bermingham and Smeaton \cite{Berm2010} who found that the MNB Classifier performed better than the SVM Classifier on tweets and short reviews. In our experiments, the SVM Classifier outperformed the MNB Classifier in all metrics. This difference in results could be due to the change in character count introduced by Twitter in 2017. The character count was increased from 140 characters to 280 characters. Bermingham and Smeaton \cite{Berm2010} found that the SVM Classifier outperformed the MNB Classifier on longer form text so perhaps the character count increase means that tweets are now more similar to longer form text. This warrants further investigation in future work.

The next best performing classifiers were the GP Classifier and the BNB Classifier, with f1-scores of 0.72. This somewhat agrees with Bermingham and Smeaton, who found that the MNB Classifier achieved the best classification results on their set of tweets. I have not come across any examples of the GP Classifier performing well for tweet classification.

\subsection{Accuracy}

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Accuracy of Classifiers for different Feature Representations.}
\label{Table:accuracy}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.703 & 0.706 & 0.696 & 0.691 & 0.721 & 0.677 & 0.635  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.614 & 0.596 & 0.614 & 0.621 & 0.652 & 0.563 & 0.594  \\ \hline
\textbf{MLP} & 0.719 & 0.719 & 0.711 & 0.711 & 0.709 & 0.718 & 0.621  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.594 & \textcolor{red}{0.744} & 0.737 & 0.718 & 0.724 & 0.716 & 0.637  \\ \hline
\textbf{LR} & 0.724 & 0.721 & 0.709 & 0.696 & 0.713 & 0.722 & 0.639  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.608 & 0.655 & 0.657 & 0.652 & 0.649 & 0.681 & 0.609  \\ \hline
\textbf{GP} & 0.726 & 0.726 & 0.724 & 0.706 & 0.708 & 0.729 & 0.644  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.654 & 0.644 & 0.650 & 0.650 & 0.655 & 0.637 & 0.609  \\ \hline
\textbf{GNB} & 0.478 & 0.483 & 0.473 & 0.448 & 0.479 & 0.565 & 0.486  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.660} & 0.683 & 0.691 & 0.675 & 0.678 & 0.569 & 0.588  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.713 & 0.713 & 0.662 & 0.621 & 0.709 & 0.565 & 0.537  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.263 & 0.235 & 0.238 & 0.558 & 0.261 & 0.612 & 0.678  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.609 & 0.614 & 0.609 & 0.588 & 0.591 & 0.693 & 0.665  \\ \hline
\end{tabular}}
\end{table}

The classifier with the highest accuracy score was also the SVM classifier with an accuracy score of 0.744 (Table ~\ref{Table:accuracy}). This indicates it had the highest percentage of correct predictions. The next best performing classifier was the GP classifier.

Overall considering the four classification metrics evaluated, the best performing classifier was the SVM Classifier, achieving the highest score in three out of four of the metrics, all but precision. The SVM classifier has a good balance of precision and recall, achieving the highest f1-score, and also had the highest accuracy score of all the classifiers.

A possible reason that SVM achieves the top performance is that the SVM classifier tends to be effective in high dimensional feature spaces. Twitter data produces a large feature set making this property of the SVM classifier useful. Another property of the SVM classifier is that they are still effective in cases where the number of dimensions is greater than the number of samples. The number of dimensions in this case, is not higher than the number of samples. However, our number of samples is relatively low.

\subsection{Feature Representation}

Different types of feature representation were explored to try and improve the performance of the classifiers. This section will outline the changes in performance produced by these feature representation methods. 

\subsubsection*{TF-IDF}
TF-IDF did not have a massive impact on the performance of the majority of the classifiers in comparison to just BOW. It improved performance in some cases and worsened performance in others. TF-IDF and BOW achieved roughly the same scores. TF-IDF did however significantly improve the classification precision, recall, accuracy and f1-scores of the SVM classifier. These scores saw an increase from about 0.5 to about 0.7.

TF-IDF does not conclusively improve tweet classification performance. This may be due to the short length of the tweets. The short text is likely to have noisy TF-IDF values whereas the binary occurrence information obtained through plain BOW is more stable.

\subsubsection*{N-Grams}
Three different N-gram feature sets were evaluated: unigrams, unigrams and bigrams, unigrams and bigrams and trigrams. We found that increasing the number of N-grams did not improve the classification precision, accuracy, recall or f1-score. In most cases, it actually worsened the performance of the classifiers or had little or no effect. This agrees with the findings of Bermingham and Smeaton \cite{Berm2010} who found that expanding N-grams did not help the classification performance of either the tweets or the short reviews they experimented with them on. 

\subsubsection*{Word2Vec and Doc2Vec}

Word2Vec achieved better results in classifying the tweets than Doc2Vec. This could be because Word2Vec used Google's pretrained model. This model was trained on about 100 billion words from a Google News dataset. In comparison, the Doc2Vec model was trained on our significantly smaller dataset of 3116 tweets.

Neither Doc2Vec or Word2Vec achieved any better performance than BOW or TF-IDF. Like BOW, Word2Vec also loses the relationships between words when documents are converted to numerical vectors. This may be a reason why Word2Vec performs no better than BOW.

\subsubsection*{Stop Words}

The unigram TF-IDF feature representation was implemented with and without stop words. The implementation with stop words included, performed better in nine out of the thirteen classifiers. This indicates that stop word removal did not improve classification performance. TF-IDF itself reflects how important a term is within a collection. It may be that TF-IDF is already accounting for the stop words high frequency, so no stop word removal is required.\newline

The best performing approach was a combination of the SVM Classifier and unigram TF-IDF feature representation, closely followed by bigram TF-IDF. TF-IDF balances out how important a term is to a document compared with how important it is to the entire dataset. It reduces the weighting of less important terms that occur in many documents in the dataset. TF-IDF seems to perform particularly well in combination with the SVM Classifier. TF-IDF significantly improved the performance of the SVM Classifier.

\section{Sentiment Scores}

The Stanford NLP Sentiment Analyser was used to generate sentiment scores for the tweets that were classified as reviews. They were aggregated and normalised, as described in chapter 3, to produce an overall sentiment score for each hotel. Each score lies between zero and one, with one being the most positive score and zero being the most negative score. The sentiment scores of the top ten highest scoring hotels (Figure ~\ref{fig:highest}) and the top ten lowest scoring hotels (Figure ~\ref{fig:lowest}) are presented below.

\begin{figure}[h!]
\centering
\includegraphics[width=0.95\textwidth]{evaluation/highest.png}
\caption{\label{fig:highest} Hotels with Highest Sentiment Scores.}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.95\textwidth]{evaluation/lowest.png}
\caption{\label{fig:lowest} Hotels with Lowest Sentiment Scores.}
\end{figure}

\section{Evaluation Metrics for Recommender Systems}

\subsection{Leave-One-Out Cross Validation}

The 'leave-one-out' cross validation approach involves removing one of a users n hotel bookings and using their remaining hotel bookings to generate the hotel recommendations. A list of hotels ranked in descending order by their predicted value is produced. The booking that was removed is compared to this list to see where it ranked, the higher the better. This is repeated with each of the users n bookings.

The 'leave-one-out' approach was used because of the data that we have. We do not have any explicit user reactions to hotel recommendations but we do have implicit feedback in the form of user hotel bookings. 

\subsection{Mean Percentile Rank}

The Mean Percentile Rank (MPR) is calculated based on the rankings recorded through leave-one-out cross validation. MPR is a recall-based measure. It measures the user satisfaction of items in an ordered list. The MPR formula is as follows:

\begin{equation}
    MPR = \frac{ \sum_{u,i} r_{ui} \times rank_{ui} } {\sum_{u,i} r_{ui}}
\end{equation}

$rank_{ui}$ is the percentile-ranking of hotel i within the ordered list of all hotels ranked for user u. $r_{ui}$ is a binary variable indicating whether user u booked hotel i. $rank_{ui} = 100\%$ indicates that the hotel i is predicted to be less desirable for user u. $rank_{ui} = 0\%$ indicates that the hotel i is predicted to be the most desirable hotel for user u. $rank_{ui} = 50\%$ would be expected for a list of randomly ranked hotels.

\section{Evaluation of Added Sentiment Scores}

\subsection{User Data}
The same Ryanair data that was used to evaluate the CoRE recommender was used for our evaluation. The Ryanair dataset consists of 29,704 hotel bookings, 11,638 unique hotels that have a least 1 booking and 20,223 users who made a booking at one or more hotels.

This project focused on hotels in Dublin so the dataset had to be filtered. The dataset was first filtered so that it only contained users with multiple hotel bookings. Multiple hotel bookings are required for the 'leave-one-out' approach so that a booking can be removed while still leaving at least one booking to use for generating the recommendations. Then the dataset was filtered so that it only contained users who had a booking in one of the Dublin hotels that we had produced a sentiment score for. The 'leave-one-out' approach had to be altered slightly so that the user's Dublin hotel booking was always the booking that was removed. The recommendations could be generated based on the other bookings but the removed booking had to be from Dublin so that we could evaluate where it ranked in the returned list.

There were 27 users who had multiple hotel bookings with at least one hotel booking in a Dublin hotel that we had a sentiment score for. The Dublin hotel bookings were for 21 different hotels. The evaluation was performed on these 27 users and 21 hotels. This is quite a small number of users which needs to be taken into consideration in the evaluation. Ideally, we would like to evaluate the system on a much larger set of users and hotels, to verify our results. This is an area for future work.

\subsection{Baselines}

In order to assess the performance of SentiCoRE (CoRE with the added sentiment information), it was compared against multiple baselines. These included: 
\begin{enumerate}
    \item \textbf{Random} \newline
    A randomly ranked list of hotels.
    \item \textbf{Expedia Baseline} \newline
    The approach currently used by Ryanair in their rooms booking site. Hotels from the target city are sorted based on a sequence number given to them by Expedia. The sequence number is based on their transactional data from the last 30 days.
    \item \textbf{CoRE} \newline
    The CoRE recommender system without the sentiment score added in.
    \item \textbf{SentiCoRE} \newline
    The CoRE recommender system re-ranked with the sentiment scores produced by the Stanford NLP Sentiment Analyser.
\end{enumerate}

\section{Recommender Results and Analysis}

The results show that the addition of the sentiment score increases the MPR of CoRE, with and without feature weighting (Table ~\ref{Table:senticore}), which means the hotels are being ranked lower by SentiCoRE than CoRE. The best performing recommender system is CoRE, achieving a MPR of 2.51\%. CoRE performs slightly better with feature weighting than without feature weighting as was seen in the evaluation of CoRE by itself \cite{core2019}. 

The Random Recommender which produces a random list of hotels performed the worst, which was expected. Typically a MPR of 50\% would be expected for a random list of hotels which is not far off the Random Recommender's MPR of 42.72\%. The Expedia Recommender only performed slightly better than the Random Recommender with a MPR of 40.87. Both CoRE and SentiCoRE performed significantly better than the Random and Expedia baselines.

\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Recommender Systems Performance.}
\label{Table:senticore}
\begin{tabular}{ll}
\specialrule{1.5pt}{1pt}{1pt}
\textbf{Recommender System} & \textbf{MPR (\%)} \\ \specialrule{1.5pt}{1pt}{1pt}
\rowcolor[HTML]{EFEFEF}
Random & 42.72 \\ \hline
Expedia Baseline & 40.87 \\ \hline
\rowcolor[HTML]{EFEFEF}
CoRE (with feature weighting) & 2.51 \\ \hline
CoRE (without feature weighting) & 3.31 \\ \hline
\rowcolor[HTML]{EFEFEF}
SentiCoRE (with feature weighting) & 14.68 \\ \hline
SentiCoRE (without feature weighting) & 9.92 \\ \hline
\end{tabular}
\end{table}

From the results, we can see that incorporating the sentiment scores into the CoRE recommender definitely has an impact on the hotel rankings. A positive sentiment score will move a hotel up the list while a negative ranking will move a hotel down the list. Taking only the MPR into account you would say that SentiCoRE is not performing as well as CoRE. However, SentiCoRE is having the desired effect and is adjusting the hotel rankings based on the sentiment scores.

\section{Summary}

One of the main aims of this research was to determine if it was possible to identify whether tweets contained reviews. A series of classifiers with different feature representations were implemented and evaluated. 

The best performing approach was a combination of the SVM Classifier and unigram TF-IDF feature representation, closely followed by bigram TF-IDF. 

The other aim of this research was to evaluate if review-like tweets could be used to appropriately influence the generation of recommendations in a recommender system. Sentiment scores were produced based on the classified tweets and used to re-rank the results of the CoRE recommender system. Incorporating the sentiment scores into the CoRE recommender had the desired effect and adjusted the rankings of the hotels. However, SentiCoRE performed worse than CoRE in terms of MPR.

