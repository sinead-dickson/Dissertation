\chapter{Evaluation}

\section{Introduction}
This chapter will outline the results of this research project. First, the evaluation metrics that were used to evaluate the classifiers will be introduced. The results of the evaluation of the thirteen classifiers with the seven different feature representations will be presented. 

\section{Evaluation Metrics for Classification}
The following four metrics were used to evaluate the classifiers:
\begin{enumerate}
    \item Precision
    \item Recall
    \item Accuracy
    \item F1-Score
\end{enumerate}

\subsection*{Precision}
Precision is the percentage of positive identifications that were classified correctly.
\begin{center}
    $Precision\ =\ \cfrac{True\ Positives}{True\ Positives\ +\ False\ Positives}$
\end{center}
The higher the precision score the better. A low precision score can indicate that there is a large number of false positives. A high precision score indicates that the majority of what was classified, was classified correctly. You could however, have a very high precision in the case where only a small portion of samples were actually classified but of these the majority were classified correctly. In this case the precision is high but the recall would be very low.

\subsection*{Recall}
Recall is the percentage of the actual positives that were classified correctly.
\begin{center}
    $Recall\ =\ \cfrac{True\ Positives}{True\ Positives\ +\ False\ Negatives}$
\end{center}
The higher the recall score the better. A low recall value can indicate there are a large number of false negatives. A high recall value indicates that the majority of what should have been classified as a was classified correctly. You could however have a very high recall and low precision if everything was classified. All the samples that that should have been classified are classified correctly, giving the high recall. But a lot of extra samples are classified that shouldn't have been classified, giving a low precision.

\subsection*{Accuracy}
Accuracy is the total number of predictions the classifier got right. It is the percentage of the tweets that were classified correctly.
\begin{center}
    $Accuracy\ =\ \cfrac{Number\ of\ Correct\ Predictions}{Total\ Number\ of\ Predictions}$
\end{center}
The higher the accuracy score the better, but a high accuracy does not always give an accurate representation of the situation if there is a major class imbalance.

\subsection*{F1-Score}
F1-Score is the harmonic mean of precision and recall. It is also called the F-Score or F-Measure. The F1-Score conveys the balance between precision and recall.
\begin{center}
    $F1-Score\ =\ 2 \times\ \cfrac{Precision\ \times\ Recall}{Precision\ +\ Recall}$
\end{center}
The higher the F1-Score the better.

\subsection*{Multi-CLass Classification}
Multi-Class Classification is a classification task that with two or more classes. This research involves a multi-class classification problem, classifying tweets as one of the three  classes 'review', 'some content', or 'irrelevant'. For a binary classification problem, the precision, recall, f1-score and accuracy can be calculated simply with the above formulas. For multi-class classification there needs to be a way of getting an average metric for the classes.
There are three ways that precision, recall, f1-score and accuracy can be calculated for this multi-class classification problem:
\begin{enumerate}
    \item \textbf{Micro Average:}\newline
    Micro Average calculates metric globally. It aggregates the contributions of all classes, counting the total TP, FN and FP, to calculate the average score.  
    \item \textbf{Macro Average:}\newline
    Macro Average calculates the metrics independently for each class and then calculates their unweighted mean.
    \item \textbf{Weighted Average:}\newline
    Weighted Average calculates the metrics independently for each class and then calculates their weighted average. This differs from Macro Average in that it takes label imbalance into account. 
\end{enumerate}

The metrics listed in the tables below (Tables ~\ref{Table:precision},~\ref{Table:recall},~\ref{Table:accuracy},~\ref{Table:f1score}) display the weighted average.

\section{Classification Results and Analysis}

In this section, the performance of the classifiers will be presented. The following thirteen classifiers were evaluated:
\begin{enumerate}
    \item Random Forest (RF) Classifier.
    \item Decision Tree (DT) Classifier.
    \item Multi Layer Perceptron (MLP) Classifier.
    \item Support Vector Machine (SVM) Classifier.
    \item Logistic Regression (LR) Classifier.
    \item K Nearest Neighbours (KNN) Classifier.
    \item Gaussian Process (GP) Classifier.
    \item AdaBoost (AB) Classifier.
    \item Gaussian Naïve Bayes (GNB) Classifier.
    \item Multinomial Naïve Bayes (MNB) Classifier.
    \item Bernouilli Naïve Bayes (BNB) Classifier.
    \item Quadratic Discriminant Analysis (QDA) Classifier.
    \item Linear Discriminant Analysis (LDA) Classifier.
\end{enumerate}

The classifiers were evaluated with seven different feature representations:
\begin{enumerate}
    \item Unigram Bag-of-Words (BOW): A unigram implementation of BOW using Scikit Learn's Count Vectorizer.
    \item Unigram TF-IDF: A unigram implementation of TFIDF using Scikit Learn's Count Vectorizer and TFIDF transformer.
    \item Bigram TF-IDF: A bigram implementation of TFIDF using Scikit Learn's Count Vectorizer and TFIDF transformer.
    \item Trigram TF-IDF: A trigram implementation of TFIDF using Scikit Learn's Count Vectorizer and TFIDF transformer.
    \item Unigram TF-IDF with stop words removed: A unigram implementation of TFIDF with stop words removed using Scikit Learn's Count Vectorizer, TFIDF transformer and English stop word list.
    \item Doc2Vec: Gensim's implementation of doc2vec, each tweet being considered a document.
    \item Word2Vec: Gensim's implementation of word2vec with Google's pretrained model.
\end{enumerate}

Each classifier and feature representation was evaluated in terms of precision, recall, accuracy and f1-score.

% *** PRECISION ***
\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Precision of Classifiers for different Feature Representations.}
\label{Table:precision}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Doc2Vec} & \textbf{Word2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.72 & 0.74 & 0.7 & 0.69 & 0.73 & 0.59 & 0.71 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.57 & 0.58 & 0.58 & 0.59 & 0.63 & 0.56 & 0.54 \\ \hline
\textbf{MLP} & 0.71 & 0.71 & 0.7 & 0.7 & 0.7 & 0.59 & 0.71 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.56 & 0.74 & 0.73 & 0.71 & 0.71 & 0.62 & 0.71 \\ \hline
\textbf{LR} & 0.72 & 0.72 & 0.7 & 0.68 & 0.7 & 0.61 & 0.71 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.53 & 0.62 & 0.62 & 0.62 & 0.61 & 0.54 & 0.67 \\ \hline
\textbf{GP} & 0.72 & 0.72 & 0.71 & 0.69 & 0.7 & 0.61 & 0.72 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.63 & 0.62 & 0.63 & 0.62 & 0.64 & 0.57 & 0.63 \\ \hline
\textbf{GNB} & 0.6 & 0.6 & 0.63 & 0.63 & 0.59 & 0.59 & 0.64 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.71} & 0.67 & 0.68 & 0.66 & 0.67 &  &  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.73 & 0.73 & 0.71 & 0.71 & 0.72 & 0.58 & 0.65 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.72 & 0.76 & 0.73 & 0.67 & \textcolor{red}{0.79} & 0.57 & 0.59 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.63 & 0.64 & 0.65 & 0.63 & 0.62 & 0.66 & 0.69 \\ \hline
\end{tabular}}
\end{table}

In terms of precision, QDA with unigram TFIDF with no stop words feature representation was the best performing classifier with 0.79 (Table ~\ref{Table:precision}). This indicates that QDA had a high percentage of positive identifications that were classified correctly. QDA with TFIDF, this time with stop words has the next highest precision score of 0.76, confirming that unigram is the way to go with QDA. RF and SVM, both with unigram TFIDF were the next best performing classifiers with precision values of 0.74.

In the study by A.Rane and A.Kumar \cite{Rane2018}, in which they classified the sentiment of twitter reviews. RF achieved the best precision score, followed by AdaBoost and then SVM. They did not include QDA in their evaluation. Our results do not fully agree with this study but they are not miles off. Two out of three of our top performing classifiers agree. Overall there precision scores are higher than ours, at about 0.8 compared to our 0.7. A possible cause of this that they trained their classifiers on a much larger dataset, a total of 14640 tweets, while our classifiers were trained on a set of 3116 tweets.

% *** RECALL ***
\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Recall of Classifiers for different Feature Representations.}
\label{Table:recall}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Doc2Vec} & \textbf{Word2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.7 & 0.71 & 0.7 & 0.69 & 0.72 & 0.64 & 0.69 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.61 & 0.6 & 0.61 & 0.62 & 0.65 & 0.59 & 0.56 \\ \hline
\textbf{MLP} & 0.72 & 0.72 & 0.71 & 0.71 & 0.71 & 0.62 & 0.72 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.59 & \textcolor{red}{0.74} & \textcolor{red}{0.74} & 0.72 & 0.72 & 0.64 & 0.72 \\ \hline
\textbf{LR} & 0.72 & 0.72 & 0.71 & 0.7 & 0.71 & 0.64 & 0.72 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.61 & 0.66 & 0.66 & 0.65 & 0.65 & 0.61 & 0.68 \\ \hline
\textbf{GP} & 0.73 & 0.73 & 0.72 & 0.71 & 0.71 & 0.64 & 0.73 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.65 & 0.64 & 0.65 & 0.65 & 0.66 & 0.61 & 0.64 \\ \hline
\textbf{GNB} & 0.48 & 0.48 & 0.47 & 0.45 & 0.48 & 0.49 & 0.56 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.66} & 0.68 & 0.69 & 0.67 & 0.68 &  &  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.71 & 0.71 & 0.66 & 0.62 & 0.71 & 0.54 & 0.56 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.26 & 0.23 & 0.24 & 0.56 & 0.26 & 0.68 & 0.61 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.61 & 0.61 & 0.61 & 0.59 & 0.59 & 0.67 & 0.69 \\ \hline
\end{tabular}}
\end{table}

The SVM classifier was the best performing in terms of recall with a recall score of 0.74 (Table ~\ref{Table:recall}). The feature representations TFIDF unigram and bigram performed equally well. This again somewhat agrees with \cite{Rane2018}, who found that SVM was their second best performing classifier, with RF coming in on top.

Notice that QDA which performed the best in precision does pretty poorly in recall. This is a common issue and is why we have also evaluated the classifiers in terms of f1-score, which shows the balance between precision and recall.

GP was the next highest preforming classifier in terms of recall with a recall score of 0.73 with unigram TFIDF feature representation. This is interesting as I have not found any studies where a GP classifier has been applied to tweets. GP performed well in precision as well achieving a precision score of 0.72.  

% *** F1 SCORE ***
\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{F1-Score of Classifiers for different Feature Representations.}
\label{Table:f1score}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Doc2Vec} & \textbf{Word2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.65 & 0.65 & 0.64 & 0.64 & 0.69 & 0.59 & 0.63 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.57 & 0.59 & 0.58 & 0.6 & 0.63 & 0.56 & 0.54 \\ \hline
\textbf{MLP} & 0.71 & 0.7 & 0.69 & 0.69 & 0.69 & 0.57 & 0.71 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.45 & \textcolor{red}{0.73} & \textcolor{red}{0.73} & 0.71 & 0.71 & 0.63 & 0.71 \\ \hline
\textbf{LR} & 0.71 & 0.7 & 0.69 & 0.68 & 0.69 & 0.6 & 0.71 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.49 & 0.62 & 0.62 & 0.62 & 0.61 & 0.55 & 0.67 \\ \hline
\textbf{GP} & 0.71 & 0.71 & 0.72 & 0.7 & 0.7 & 0.61 & 0.72 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.63 & 0.62 & 0.63 & 0.62 & 0.63 & 0.59 & 0.63 \\ \hline
\textbf{GNB} & 0.5 & 0.51 & 0.49 & 0.47 & 0.5 & 0.51 & 0.58 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.67} & 0.63 & 0.66 & 0.65 & 0.64 &  &  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.72 & 0.72 & 0.67 & 0.64 & 0.71 & 0.55 & 0.58 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.23 & 0.19 & 0.19 & 0.48 & 0.23 & 0.61 & 0.55 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.62 & 0.62 & 0.62 & 0.6 & 0.6 & 0.66 & 0.69 \\ \hline
\end{tabular}}
\end{table}

Again, the SVM classifier achieved the best f1-score of 0.73 (Table ~\ref{Table:f1score}), with the TFIDF unigram and bigram feature representations again performing equally well. This indicates that the SVM classifier has a good balance of precision and recall.This confirms the results of both M. Rathi, A. Malik, D. Varshney, R. Sharma, and S. Mendiratta \cite{Raithi2018}, and A. Rane and A. Kumar \cite{Rane2018} who both found that the SVM classifier performed well in classifying tweets. However, it disagrees with the results of A. Bermingham and A. Smeaton who found that MNB performed better than SVM on tweets and short reviews. In this experiment SVM outperformed MNB in all metrics.

One reason that SVM performs the best could be that in general the SVM classifier is effective in high dimensional feature spaces. Out Twitter data produces a large feature set so this property of SVMs is useful. SVMs are still effective in cases where the number of dimensions is greater than the number of samples. The number of dimensions in this case is not higher than the number of samples. However the number of samples is relatively low.

The next best performing classifiers were GP and BNB, with f1-scores of 0.72. This somewhat agrees with A. Bermingham and A. Smeaton, who found that MNB achieved the best classification results on their set of tweets. I have not come across any example of GP performing well for tweet classification.

% *** ACCURACY ***
\begin{table}[h!]
\setlength\extrarowheight{5pt}
\caption{Accuracy of Classifiers for different Feature Representations.}
\label{Table:accuracy}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.5pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TFIDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Doc2Vec} & \textbf{Word2Vec} \\ \specialrule{1.5pt}{1pt}{1pt}
\textbf{RF} & 0.703 & 0.706 & 0.696 & 0.691 & 0.721 & 0.635 & 0.677 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.614 & 0.596 & 0.614 & 0.621 & 0.652 & 0.594 & 0.563 \\ \hline
\textbf{MLP} & 0.719 & 0.719 & 0.711 & 0.711 & 0.709 & 0.621 & 0.718 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.594 & \textcolor{red}{0.744} & 0.737 & 0.718 & 0.724 & 0.637 & 0.716 \\ \hline
\textbf{LR} & 0.724 & 0.721 & 0.709 & 0.696 & 0.713 & 0.639 & 0.722 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.608 & 0.655 & 0.657 & 0.652 & 0.649 & 0.609 & 0.681 \\ \hline
\textbf{GP} & 0.726 & 0.726 & 0.724 & 0.706 & 0.708 & 0.644 & 0.729 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.654 & 0.644 & 0.650 & 0.650 & 0.655 & 0.609 & 0.637 \\ \hline
\textbf{GNB} & 0.478 & 0.483 & 0.473 & 0.448 & 0.479 & 0.486 & 0.565 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.660} & 0.683 & 0.691 & 0.675 & 0.678 &  &  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.713 & 0.713 & 0.662 & 0.621 & 0.709 & 0.537 & 0.565 \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.263 & 0.235 & 0.238 & 0.558 & 0.261 & 0.678 & 0.612 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.609 & 0.614 & 0.609 & 0.588 & 0.591 & 0.665 & 0.693 \\ \hline
\end{tabular}}
\end{table}

The classifier with the highest accuracy score was the SVM classifier with an accuracy score of 0.744 (Table ~\ref{Table:accuracy}).

Overall taking the four metrics into account the best performing classifier was the SVM classifier, achieving the highest score in three out of four of the metrics, all but precision. It has a good balance of precision and recall and also has a high accuracy score.

\subsection*{Feature Representations}

\subsubsection*{N-Grams}
Three different N-gram feature sets were evaluated: unigrams, unigrams and bigrams, unigrams and bigrams and trigrams. We found that increasing the number of N-grams did not improve the classification precision, accuracy, recall or f1-score. In most cases it actually worsened the performance of the classifiers or had little to no affect. This agrees with the findings of A. Bermingham and A. Smeaton \cite{Berm2010} who that expanding N-grams did not help the classification performance of either the tweets or the short reviews they tested them on. 




\section{Evaluation Metrics for Recommender Systems}

\section{Recommender Results}

\section{Discussion of Results}

The best performing classifier was the Support Vector Machine.

\section{Summary}

\subsection*{Limitations}
